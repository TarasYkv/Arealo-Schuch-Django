"""
Content Service f√ºr BlogPrep

Verantwortlich f√ºr:
- Web-Recherche und Konkurrenzanalyse
- Gliederung erstellen
- Blog-Content generieren (Intro, Main, Tips)
- FAQs generieren
- SEO-Meta generieren
"""

import logging
import json
import re
import time
from typing import Dict, List, Optional, Any

# Optionale Imports - werden nur bei Bedarf geladen
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OpenAI = None
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    genai = None
    GENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    anthropic = None
    ANTHROPIC_AVAILABLE = False

logger = logging.getLogger(__name__)


def _get_user_friendly_error(error: Exception, provider: str) -> str:
    """
    Wandelt API-Fehler in benutzerfreundliche deutsche Fehlermeldungen um.
    """
    error_str = str(error).lower()

    # OpenAI spezifische Fehler
    if 'insufficient_quota' in error_str or 'exceeded your current quota' in error_str:
        return (
            f"‚ö†Ô∏è OpenAI API-Guthaben ersch√∂pft!\n\n"
            f"M√∂gliche Ursachen:\n"
            f"‚Ä¢ Kein Guthaben auf dem OpenAI-Konto\n"
            f"‚Ä¢ Keine Zahlungsmethode hinterlegt\n"
            f"‚Ä¢ Projekt-Budget-Limit erreicht\n"
            f"‚Ä¢ Trial-Credits aufgebraucht\n\n"
            f"L√∂sung:\n"
            f"1. √ñffne https://platform.openai.com/account/billing/overview\n"
            f"2. Pr√ºfe ob Guthaben vorhanden ist oder f√ºge Zahlungsmethode hinzu\n"
            f"3. Pr√ºfe Projekt-Limits unter Settings ‚Üí Limits\n\n"
            f"Alternativ: Wechsle zu Gemini oder Anthropic in den Einstellungen."
        )

    if 'rate_limit' in error_str or 'rate limit' in error_str:
        return (
            f"‚è≥ Rate-Limit erreicht ({provider})\n\n"
            f"Du hast zu viele Anfragen in kurzer Zeit gesendet.\n"
            f"Warte 1-2 Minuten und versuche es erneut."
        )

    if 'invalid_api_key' in error_str or 'invalid api key' in error_str or 'incorrect api key' in error_str:
        return (
            f"üîë Ung√ºltiger API-Key ({provider})\n\n"
            f"Der gespeicherte API-Key ist ung√ºltig oder wurde widerrufen.\n"
            f"Bitte aktualisiere deinen API-Key in den Profil-Einstellungen."
        )

    if 'authentication' in error_str or 'unauthorized' in error_str:
        return (
            f"üîê Authentifizierungsfehler ({provider})\n\n"
            f"Der API-Key konnte nicht authentifiziert werden.\n"
            f"Pr√ºfe deinen API-Key in den Profil-Einstellungen."
        )

    if 'connection' in error_str or 'timeout' in error_str or 'network' in error_str:
        return (
            f"üåê Verbindungsfehler ({provider})\n\n"
            f"Konnte keine Verbindung zum API-Server herstellen.\n"
            f"Bitte versuche es in einigen Sekunden erneut."
        )

    if 'model_not_found' in error_str or 'model not found' in error_str:
        return (
            f"ü§ñ Modell nicht verf√ºgbar ({provider})\n\n"
            f"Das gew√§hlte KI-Modell ist nicht verf√ºgbar.\n"
            f"W√§hle ein anderes Modell in den Einstellungen."
        )

    # Generischer Fehler
    return f"‚ùå API-Fehler ({provider}): {str(error)}"


class ContentService:
    """Service f√ºr KI-gest√ºtzte Content-Generierung"""

    def __init__(self, user, settings=None):
        """
        Initialisiert den Content Service.

        Args:
            user: Django User Objekt mit API-Keys
            settings: Optional BlogPrepSettings Objekt
        """
        self.user = user
        self.settings = settings

        # Provider und Modell aus Settings oder Defaults
        if settings:
            self.provider = settings.ai_provider
            self.model = settings.ai_model
        else:
            self.provider = 'openai'
            self.model = 'gpt-4o'

        # Clients initialisieren
        self._init_clients()

    def _init_clients(self):
        """Initialisiert die API-Clients basierend auf Provider"""
        self.openai_client = None
        self.gemini_model = None
        self.anthropic_client = None

        if self.provider == 'openai' and OPENAI_AVAILABLE and self.user.openai_api_key:
            self.openai_client = OpenAI(api_key=self.user.openai_api_key)
        elif self.provider == 'gemini' and GENAI_AVAILABLE and self.user.gemini_api_key:
            genai.configure(api_key=self.user.gemini_api_key)
            self.gemini_model = genai.GenerativeModel(self.model)
        elif self.provider == 'anthropic' and ANTHROPIC_AVAILABLE and self.user.anthropic_api_key:
            self.anthropic_client = anthropic.Anthropic(api_key=self.user.anthropic_api_key)

    def _call_llm(self, system_prompt: str, user_prompt: str, max_tokens: int = 2000, temperature: float = 0.7) -> Dict:
        """
        Ruft das LLM auf basierend auf dem konfigurierten Provider.

        Returns:
            Dict mit 'success', 'content' oder 'error'
        """
        start_time = time.time()

        try:
            if self.provider == 'openai' and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                content = response.choices[0].message.content.strip()
                tokens_in = response.usage.prompt_tokens if response.usage else 0
                tokens_out = response.usage.completion_tokens if response.usage else 0

            elif self.provider == 'gemini' and self.gemini_model:
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                response = self.gemini_model.generate_content(
                    full_prompt,
                    generation_config=genai.GenerationConfig(
                        max_output_tokens=max_tokens,
                        temperature=temperature
                    )
                )
                content = response.text.strip()
                tokens_in = 0  # Gemini gibt keine Token-Info
                tokens_out = 0

            elif self.provider == 'anthropic' and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model=self.model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                content = response.content[0].text.strip()
                tokens_in = response.usage.input_tokens if response.usage else 0
                tokens_out = response.usage.output_tokens if response.usage else 0

            else:
                return {
                    'success': False,
                    'error': f'API-Key f√ºr {self.provider} nicht konfiguriert'
                }

            duration = time.time() - start_time

            return {
                'success': True,
                'content': content,
                'tokens_input': tokens_in,
                'tokens_output': tokens_out,
                'duration': duration
            }

        except Exception as e:
            logger.error(f"LLM call error ({self.provider}): {e}")
            user_friendly_error = _get_user_friendly_error(e, self.provider)
            return {
                'success': False,
                'error': user_friendly_error
            }

    def _parse_json_response(self, content: str) -> Optional[Dict]:
        """Versucht JSON aus der LLM-Antwort zu extrahieren"""
        try:
            # Versuche direkt zu parsen
            return json.loads(content)
        except json.JSONDecodeError:
            # Versuche JSON aus Markdown-Codeblock zu extrahieren
            json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', content)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except json.JSONDecodeError:
                    pass

            # Versuche JSON-Objekt zu finden
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass

        return None

    def analyze_research(self, keyword: str, search_results: List[Dict]) -> Dict:
        """
        Analysiert echte Web-Recherche-Ergebnisse und extrahiert wichtige Informationen.

        Args:
            keyword: Das Hauptkeyword
            search_results: Liste von Suchergebnissen mit title, snippet, url, content

        Returns:
            Dict mit topics, related_keywords, questions
        """
        system_prompt = """Du bist ein SEO-Experte und Content-Stratege.
Analysiere die echten Suchergebnisse aus dem Web und extrahiere die wichtigsten Informationen f√ºr einen Blogbeitrag.
Du erh√§ltst die tats√§chlichen Inhalte der Top-Suchergebnisse."""

        # Formatiere Suchergebnisse mit echtem Inhalt
        results_text = ""
        for i, result in enumerate(search_results[:5], 1):
            results_text += f"\n{'='*50}\n"
            results_text += f"SUCHERGEBNIS {i}:\n"
            results_text += f"Titel: {result.get('title', '')}\n"
            results_text += f"URL: {result.get('url', '')}\n"
            results_text += f"Snippet: {result.get('snippet', '')}\n"

            # F√ºge den extrahierten Seiteninhalt hinzu
            content = result.get('content', '') or result.get('content_preview', '')
            if content:
                # Begrenze den Inhalt pro Ergebnis
                content_preview = content[:1500] + '...' if len(content) > 1500 else content
                results_text += f"\nSEITENINHALT:\n{content_preview}\n"

        # Falls keine Ergebnisse, nur Keyword-basierte Analyse
        if not results_text.strip():
            results_text = f"(Keine Suchergebnisse verf√ºgbar - analysiere basierend auf Keyword '{keyword}')"

        user_prompt = f"""Analysiere diese echten Web-Suchergebnisse zum Keyword "{keyword}":

{results_text}

WICHTIG: Basiere deine Analyse auf den TATS√ÑCHLICHEN INHALTEN der gefundenen Seiten.
Identifiziere:
- Welche Themen werden von den Top-Ergebnissen behandelt?
- Welche Keywords/Begriffe werden h√§ufig verwendet?
- Welche Fragen werden beantwortet?
- Was fehlt in den bestehenden Artikeln?

Erstelle eine strukturierte Analyse als JSON:

{{
    "main_topics": ["Die 5-7 wichtigsten Themen die in den Ergebnissen behandelt werden"],
    "related_keywords": ["10-15 verwandte Keywords und Begriffe aus den Inhalten"],
    "user_questions": ["8-10 Fragen die die Artikel beantworten oder die Nutzer haben k√∂nnten"],
    "pain_points": ["5 Hauptprobleme/Herausforderungen die angesprochen werden"],
    "content_gaps": ["Was fehlt in den bestehenden Artikeln? Was k√∂nnten wir besser machen?"],
    "key_facts": ["5-8 wichtige Fakten/Statistiken/Tipps aus den Artikeln"]
}}

Antworte NUR mit dem JSON-Objekt."""

        result = self._call_llm(system_prompt, user_prompt, max_tokens=2000)

        if not result['success']:
            return result

        parsed = self._parse_json_response(result['content'])
        if parsed:
            return {
                'success': True,
                'data': parsed,
                'tokens_input': result.get('tokens_input', 0),
                'tokens_output': result.get('tokens_output', 0),
                'duration': result.get('duration', 0)
            }

        return {
            'success': False,
            'error': 'Konnte JSON nicht parsen',
            'raw_content': result['content']
        }

    def generate_outline(self, keyword: str, secondary_keywords: List[str], research_data: Dict) -> Dict:
        """
        Generiert eine ausf√ºhrliche Blog-Gliederung basierend auf Keyword und Recherche.

        Args:
            keyword: Hauptkeyword
            secondary_keywords: Liste sekund√§rer Keywords
            research_data: Ergebnisse der Recherche-Analyse

        Returns:
            Dict mit outline (Liste von H2-√úberschriften)
        """
        system_prompt = """Du bist ein erfahrener Content-Stratege und SEO-Experte.
Erstelle eine AUSF√úHRLICHE und DETAILLIERTE Blog-Gliederung basierend auf der Web-Recherche.
Deine Gliederung muss die Erkenntnisse aus der Konkurrenzanalyse aufgreifen und BESSER sein als die analysierten Artikel."""

        topics = research_data.get('main_topics', [])
        questions = research_data.get('user_questions', [])
        key_facts = research_data.get('key_facts', [])
        pain_points = research_data.get('pain_points', [])
        content_gaps = research_data.get('content_gaps', [])
        related_keywords = research_data.get('related_keywords', [])

        user_prompt = f"""Erstelle eine AUSF√úHRLICHE Blog-Gliederung zum Thema "{keyword}".

=== RECHERCHE-ERGEBNISSE (aus Top-5 Suchergebnissen) ===

HAUPTTHEMEN DIE BEHANDELT WERDEN:
{chr(10).join(f'- {t}' for t in topics) if topics else '- Keine Daten'}

WICHTIGE FAKTEN & ERKENNTNISSE:
{chr(10).join(f'- {f}' for f in key_facts) if key_facts else '- Keine Daten'}

H√ÑUFIGE NUTZERFRAGEN:
{chr(10).join(f'- {q}' for q in questions) if questions else '- Keine Daten'}

PROBLEME & HERAUSFORDERUNGEN DER ZIELGRUPPE:
{chr(10).join(f'- {p}' for p in pain_points) if pain_points else '- Keine Daten'}

CONTENT-L√úCKEN (was die Konkurrenz NICHT gut macht):
{chr(10).join(f'- {g}' for g in content_gaps) if content_gaps else '- Keine Daten'}

VERWANDTE KEYWORDS:
{', '.join(related_keywords[:10]) if related_keywords else 'Keine'}

SEKUND√ÑRE KEYWORDS:
{', '.join(secondary_keywords) if secondary_keywords else 'Keine'}

=== ANFORDERUNGEN AN DIE GLIEDERUNG ===

1. NUTZE DIE RECHERCHE-ERGEBNISSE:
   - Greife die identifizierten Themen auf
   - Beantworte die h√§ufigen Nutzerfragen
   - Adressiere die Pain Points der Zielgruppe
   - Schlie√üe die Content-L√ºcken der Konkurrenz

2. STRUKTUR:
   - 5-7 H2-√úberschriften (mehr Detail = besser)
   - Gesamtl√§nge: ca. 1600-1800 W√∂rter (Einleitung ~400, Hauptteil ~600, Tipps ~600)
   - Lesedauer: 6-8 Minuten
   - Jeder Abschnitt mit 3-5 konkreten Key Points

3. ABSCHNITTE:
   - Einleitung (~400 W√∂rter): Pers√∂nlicher Einstieg, Problem aufgreifen, Mehrwert versprechen
   - Hauptteil (~600 W√∂rter): Grundlagen, Empfehlungen, Vergleiche
   - Tipps & Do's/Don'ts (~600 W√∂rter): Praktische Anleitungen, h√§ufige Fehler
   - Fazit: Zusammenfassung + Call-to-Action

4. SEO-OPTIMIERUNG:
   - Keyword "{keyword}" in mindestens 2 H2-√úberschriften
   - Sekund√§re Keywords nat√ºrlich einbauen
   - √úberschriften als Fragen formulieren (wo sinnvoll)

Erstelle als JSON:
{{
    "outline": [
        {{
            "h2": "Konkrete, aussagekr√§ftige √úberschrift",
            "section": "intro|main|tips",
            "word_target": 400,
            "key_points": ["Detaillierter Punkt 1", "Detaillierter Punkt 2", "Detaillierter Punkt 3"],
            "research_reference": "Welche Recherche-Erkenntnisse hier einflie√üen"
        }}
    ],
    "total_word_target": 1600,
    "unique_angle": "Was macht diesen Artikel besser als die Konkurrenz?"
}}

Antworte NUR mit dem JSON-Objekt."""

        result = self._call_llm(system_prompt, user_prompt, max_tokens=2500)

        if not result['success']:
            return result

        parsed = self._parse_json_response(result['content'])
        if parsed:
            return {
                'success': True,
                'data': parsed,
                'tokens_input': result.get('tokens_input', 0),
                'tokens_output': result.get('tokens_output', 0),
                'duration': result.get('duration', 0)
            }

        return {
            'success': False,
            'error': 'Konnte JSON nicht parsen',
            'raw_content': result['content']
        }

    def generate_content_section(
        self,
        section_type: str,
        keyword: str,
        outline_section: Dict,
        company_info: Dict,
        writing_style: str = 'du'
    ) -> Dict:
        """
        Generiert einen Content-Abschnitt (Intro, Main, Tips).

        Args:
            section_type: 'intro', 'main', oder 'tips'
            keyword: Hauptkeyword
            outline_section: Gliederung f√ºr diesen Abschnitt
            company_info: Unternehmensinformationen
            writing_style: 'du', 'sie', oder 'neutral'

        Returns:
            Dict mit content (HTML-formatierter Text)
        """
        style_instructions = {
            'du': 'Schreibe pers√∂nlich in der "du"-Form. Sei nahbar und freundlich.',
            'sie': 'Schreibe formell in der "Sie"-Form. Bleibe professionell.',
            'neutral': 'Schreibe neutral ohne direkte Anrede. Verwende passive Formulierungen.'
        }

        # Produktlinks formatieren f√ºr Prompts
        product_links = company_info.get('product_links', [])
        product_links_text = ""
        if product_links:
            product_links_text = "\n\nPRODUKTVERLINKUNGEN (f√ºge diese nat√ºrlich im Text ein wo passend):\n"
            for p in product_links:
                product_links_text += f'- <a href="{p.get("url", "")}">{p.get("name", "")}</a>\n'
            product_links_text += "\nWICHTIG: Verlinke mindestens 1-2 Produkte nat√ºrlich im Text, wenn sie thematisch passen!"

        section_prompts = {
            'intro': f"""Schreibe die EINLEITUNG (ca. 400 W√∂rter) f√ºr einen Blogbeitrag zum Thema "{keyword}".

√úBERSCHRIFT: {outline_section.get('h2', '')}
WICHTIGE PUNKTE: {', '.join(outline_section.get('key_points', []))}

ANFORDERUNGEN:
- Pers√∂nlicher Einstieg mit eigener Erfahrung aus Sicht des Unternehmens
- Erkl√§re warum das Thema wichtig ist
- Nenne Kriterien f√ºr gute Entscheidungen
- Verwende Aufz√§hlungen und fette wichtige Begriffe
- Baue das Keyword nat√ºrlich ein (2-3 mal)
- WICHTIG: Halte dich an max. 400 W√∂rter!

UNTERNEHMEN: {company_info.get('name', '')}
EXPERTISE: {company_info.get('expertise', '')}{product_links_text}

{style_instructions.get(writing_style, style_instructions['du'])}

Formatiere als HTML mit <p>, <ul>, <li>, <strong>, <a> Tags.
Beginne DIREKT mit dem Content, keine √úberschrift.""",

            'main': f"""Schreibe den HAUPTTEIL (ca. 600 W√∂rter) f√ºr einen Blogbeitrag zum Thema "{keyword}".

√úBERSCHRIFT: {outline_section.get('h2', '')}
WICHTIGE PUNKTE: {', '.join(outline_section.get('key_points', []))}

ANFORDERUNGEN:
- Pr√§sentiere konkrete Empfehlungen/Informationen
- Unterteile in √ºbersichtliche Kategorien
- F√ºge eine Vergleichstabelle ein (HTML <table>)
- Verweise subtil auf Produkte/Dienstleistungen des Unternehmens
- Begr√ºnde Empfehlungen mit Fakten oder Erfahrung
- WICHTIG: Halte dich an max. 600 W√∂rter!

UNTERNEHMEN: {company_info.get('name', '')}
PRODUKTE: {company_info.get('products', '')}{product_links_text}

{style_instructions.get(writing_style, style_instructions['du'])}

Formatiere als HTML mit <p>, <ul>, <li>, <strong>, <table>, <h3>, <a> Tags.
Beginne DIREKT mit dem Content, keine H2-√úberschrift (die kommt separat).""",

            'tips': f"""Schreibe den TIPPS-BEREICH (ca. 600 W√∂rter) f√ºr einen Blogbeitrag zum Thema "{keyword}".

√úBERSCHRIFT: {outline_section.get('h2', '')}
WICHTIGE PUNKTE: {', '.join(outline_section.get('key_points', []))}

ANFORDERUNGEN:
- Detaillierte Liste von Do's und Don'ts
- Erkl√§re h√§ufige Fehler und wie man sie vermeidet
- Gib wertvolle, praktische Tipps
- Nutze Tabellen f√ºr √úbersichtlichkeit
- Teile pers√∂nliche Erfahrungen
- WICHTIG: Halte dich an max. 600 W√∂rter!
{product_links_text}

{style_instructions.get(writing_style, style_instructions['du'])}

Formatiere als HTML mit <p>, <ul>, <li>, <strong>, <table>, <h3>, <a> Tags.
Beginne DIREKT mit dem Content, keine H2-√úberschrift."""
        }

        system_prompt = f"""Du bist ein erfahrener Content-Writer der informative, pers√∂nliche Blogbeitr√§ge schreibt.
Du schreibst aus der Perspektive von "{company_info.get('name', 'einem Experten')}" und teilst echte Erfahrungen.
Dein Stil ist {writing_style}-Form, informativ aber nicht werblich."""

        user_prompt = section_prompts.get(section_type, section_prompts['main'])

        result = self._call_llm(system_prompt, user_prompt, max_tokens=2500, temperature=0.8)

        if result['success']:
            return {
                'success': True,
                'content': result['content'],
                'tokens_input': result.get('tokens_input', 0),
                'tokens_output': result.get('tokens_output', 0),
                'duration': result.get('duration', 0)
            }

        return result

    def generate_faqs(self, keyword: str, research_data: Dict, content_summary: str = '') -> Dict:
        """
        Generiert 5 FAQs zum Thema.

        Args:
            keyword: Hauptkeyword
            research_data: Recherche-Daten mit user_questions
            content_summary: Zusammenfassung des bisherigen Contents

        Returns:
            Dict mit faqs Liste [{question, answer}]
        """
        questions = research_data.get('user_questions', [])

        system_prompt = """Du bist ein Experte f√ºr FAQ-Erstellung die sowohl nutzerfreundlich als auch SEO-optimiert sind.
Erstelle FAQs die echte Nutzerfragen beantworten und f√ºr Featured Snippets optimiert sind."""

        user_prompt = f"""Erstelle 5 FAQs zum Thema "{keyword}".

H√ÑUFIGE NUTZERFRAGEN AUS DER RECHERCHE:
{chr(10).join(f'- {q}' for q in questions)}

ANFORDERUNGEN:
- 5 relevante Fragen die Nutzer wirklich haben
- Ausf√ºhrliche, hilfreiche Antworten (je 50-100 W√∂rter)
- Einfache, verst√§ndliche Sprache
- Keyword nat√ºrlich integrieren
- Schema.org FAQ-optimiert

Erstelle als JSON:
{{
    "faqs": [
        {{
            "question": "Die Frage?",
            "answer": "Die ausf√ºhrliche Antwort..."
        }}
    ]
}}

Antworte NUR mit dem JSON-Objekt."""

        result = self._call_llm(system_prompt, user_prompt, max_tokens=2000)

        if not result['success']:
            return result

        parsed = self._parse_json_response(result['content'])
        if parsed and 'faqs' in parsed:
            return {
                'success': True,
                'faqs': parsed['faqs'],
                'tokens_input': result.get('tokens_input', 0),
                'tokens_output': result.get('tokens_output', 0),
                'duration': result.get('duration', 0)
            }

        return {
            'success': False,
            'error': 'Konnte FAQs nicht parsen',
            'raw_content': result['content']
        }

    def generate_seo_meta(self, keyword: str, content_summary: str) -> Dict:
        """
        Generiert SEO-Titel, Meta-Description und Zusammenfassung.

        Args:
            keyword: Hauptkeyword
            content_summary: Kurze Zusammenfassung des Contents

        Returns:
            Dict mit title, description, summary
        """
        system_prompt = """Du bist ein SEO-Spezialist der perfekte Meta-Daten f√ºr Suchmaschinen erstellt.
Deine Titel und Descriptions erzielen hohe Klickraten."""

        user_prompt = f"""Erstelle SEO-Meta-Daten f√ºr einen Blogbeitrag zum Thema "{keyword}".

CONTENT-ZUSAMMENFASSUNG:
{content_summary[:500] if content_summary else 'Informativer Blogbeitrag zum Thema'}

ANFORDERUNGEN:
- SEO-Titel: Max. 60 Zeichen, Keyword am Anfang, clickbait-frei aber ansprechend
- Meta-Description: Max. 155 Zeichen, Call-to-Action, Keyword enthalten
- Zusammenfassung: 50-70 W√∂rter f√ºr Social Media Sharing

Erstelle als JSON:
{{
    "title": "Der SEO-optimierte Titel",
    "description": "Die Meta-Description...",
    "summary": "Die pr√§gnante Zusammenfassung f√ºr Social Media..."
}}

Antworte NUR mit dem JSON-Objekt."""

        result = self._call_llm(system_prompt, user_prompt, max_tokens=500)

        if not result['success']:
            return result

        parsed = self._parse_json_response(result['content'])
        if parsed:
            return {
                'success': True,
                'data': parsed,
                'tokens_input': result.get('tokens_input', 0),
                'tokens_output': result.get('tokens_output', 0),
                'duration': result.get('duration', 0)
            }

        return {
            'success': False,
            'error': 'Konnte SEO-Meta nicht parsen',
            'raw_content': result['content']
        }

    def suggest_keywords(self, main_keyword: str) -> Dict:
        """
        Schl√§gt sekund√§re Keywords basierend auf dem Hauptkeyword vor.

        Args:
            main_keyword: Das Hauptkeyword

        Returns:
            Dict mit keywords Liste
        """
        system_prompt = """Du bist ein SEO-Keyword-Experte.
Schlage relevante sekund√§re Keywords vor die semantisch zum Hauptkeyword passen."""

        user_prompt = f"""Schlage 8-10 sekund√§re Keywords f√ºr "{main_keyword}" vor.

KRITERIEN:
- Semantisch verwandt
- Suchintention √§hnlich
- Mix aus Long-Tail und Short-Tail
- In Deutschland gebr√§uchlich

Erstelle als JSON:
{{
    "keywords": ["keyword1", "keyword2", ...]
}}

Antworte NUR mit dem JSON-Objekt."""

        result = self._call_llm(system_prompt, user_prompt, max_tokens=300)

        if not result['success']:
            return result

        parsed = self._parse_json_response(result['content'])
        if parsed and 'keywords' in parsed:
            return {
                'success': True,
                'keywords': parsed['keywords']
            }

        return {
            'success': False,
            'error': 'Konnte Keywords nicht parsen'
        }
